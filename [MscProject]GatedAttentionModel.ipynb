{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Telescope-U/Video-Comment-Generator/blob/master/%5BMscProject%5DGatedAttentionModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89805564"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchtext\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import math\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "from tqdm import tqdm"
      ],
      "id": "89805564"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3yyKyXwYG2Y",
        "outputId": "c961ff0e-c0dd-4670-d407-69ffee671d0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "id": "A3yyKyXwYG2Y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKfgl4DWVrae",
        "outputId": "829bccfc-98ca-45fb-94f8-9bb717096669"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "fKfgl4DWVrae"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "853e789b"
      },
      "outputs": [],
      "source": [
        "WORK_DIR = '/content/drive/MyDrive/Colab Notebooks/[Msc]Video-Comment-Generator/'\n",
        "if not os.path.isdir(WORK_DIR):\n",
        "    WORK_DIR = ''\n",
        "TRAIN_FOLDER = WORK_DIR + 'Dataset/Train/'\n",
        "VALID_FOLDER = WORK_DIR + 'Dataset/Valid/'\n",
        "TEST_FOLDER = WORK_DIR + 'Dataset/Test/'\n",
        "VIDEO_PATH = WORK_DIR + 'Dataset/video_data.csv'\n",
        "COMMENT_PATH = WORK_DIR + 'Dataset/comment_data.csv'"
      ],
      "id": "853e789b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a90612b"
      },
      "source": [
        "# 1. Dataset"
      ],
      "id": "7a90612b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "288bbd13"
      },
      "outputs": [],
      "source": [
        "def split_ids(video_ids, train=0.7, valid=0.1, test=0.2):\n",
        "    list_copy = video_ids.copy()\n",
        "    random.shuffle(list_copy)\n",
        "    train_size = math.floor(len(list_copy)*train)\n",
        "    valid_size = math.floor(len(list_copy)*valid)\n",
        "    return list_copy[:train_size], list_copy[train_size:(train_size+valid_size)], list_copy[(train_size+valid_size):]"
      ],
      "id": "288bbd13"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "002ab143"
      },
      "outputs": [],
      "source": [
        "video_df = pd.read_csv(VIDEO_PATH)\n",
        "comment_df = pd.read_csv(COMMENT_PATH)\n",
        "train_ids, valid_ids, test_ids = split_ids(video_df['vid'].unique())\n",
        "print(len(train_ids), len(valid_ids), len(test_ids))"
      ],
      "id": "002ab143"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fd3f1bff"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "def clean_text(text):\n",
        "    text = str(text)\n",
        "    for i in string.punctuation:\n",
        "        text = text.replace(i, '')\n",
        "    return text.lower()\n",
        "\n",
        "video_df['title'] = video_df['title'].apply(clean_text)\n",
        "video_df['transcript'] = video_df['transcript'].apply(clean_text)\n",
        "comment_df['en_content'] = comment_df['en_content'].apply(clean_text)"
      ],
      "id": "fd3f1bff"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "751dc8d5"
      },
      "outputs": [],
      "source": [
        "# Split and save dataset\n",
        "train_video_df = video_df[video_df['vid'].isin(train_ids)]\n",
        "valid_video_df = video_df[video_df['vid'].isin(valid_ids)]\n",
        "test_video_df = video_df[video_df['vid'].isin(test_ids)]\n",
        "\n",
        "train_comment_df = comment_df[comment_df['vid'].isin(train_ids)]\n",
        "valid_comment_df = comment_df[comment_df['vid'].isin(valid_ids)]\n",
        "test_comment_df = comment_df[comment_df['vid'].isin(test_ids)]\n",
        "\n",
        "train_comment_df.to_csv('Dataset/Train/comment.csv', index = None)\n",
        "test_comment_df.to_csv('Dataset/Test/comment.csv', index = None)\n",
        "valid_comment_df.to_csv(\"Dataset/Valid/comment.csv\", index = None)\n",
        "\n",
        "train_video_df.to_csv('Dataset/Train/video.csv', index = None)\n",
        "test_video_df.to_csv('Dataset/Test/video.csv', index = None)\n",
        "valid_video_df.to_csv(\"Dataset/Valid/video.csv\", index = None)"
      ],
      "id": "751dc8d5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3510497a"
      },
      "outputs": [],
      "source": [
        "def get_data(folder):\n",
        "    comment_path = os.path.join(folder,'comment.csv')\n",
        "    video_path = os.path.join(folder,'video.csv')\n",
        "    comment_df = pd.read_csv(comment_path)\n",
        "    video_df = pd.read_csv(video_path)\n",
        "    return {'comment':comment_df, 'video': video_df}\n",
        "        "
      ],
      "id": "3510497a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "374e4e20"
      },
      "outputs": [],
      "source": [
        "train_data = get_data(TRAIN_FOLDER)\n",
        "valid_data = get_data(VALID_FOLDER)\n",
        "test_data = get_data(TEST_FOLDER)"
      ],
      "id": "374e4e20"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3326e42"
      },
      "source": [
        "## 1.1 Vocabulary"
      ],
      "id": "e3326e42"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "390e1ce4"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "specials = ['<unk>','<pad>', '<sos>', '<eos>']\n",
        "\n",
        "word_list = []\n",
        "\n",
        "def yield_tokens():\n",
        "    for data in [train_data, valid_data]:\n",
        "        columns = [data['comment']['en_content'],\n",
        "                data['video']['title'],\n",
        "                data['video']['transcript']]\n",
        "\n",
        "        token_lists = [tokenizer(str(text)) for column in columns for text in column]\n",
        "        for tokens in token_lists:\n",
        "            yield tokens\n",
        "        \n",
        "vocabulary = build_vocab_from_iterator(yield_tokens(), specials=specials, min_freq=2)\n",
        "vocabulary.set_default_index(vocabulary['<unk>'])"
      ],
      "id": "390e1ce4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ada0bfcb",
        "outputId": "83516f7b-da89-4b38-c0b8-5ec8f57dc253",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50437"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "len(vocabulary)"
      ],
      "id": "ada0bfcb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "516e9085",
        "outputId": "aee89a1b-da97-4421-98bf-e0eb700b6bad"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[6, 166, 198]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "sentence = \"i am happy\".split()\n",
        "indexs = vocabulary.forward(sentence)\n",
        "indexs"
      ],
      "id": "516e9085"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TUGyWGuuPSr",
        "outputId": "c70acae3-3e6b-412b-9bf1-1ee14fcff20b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 0, 2]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "vocabulary.forward(['<eos>', '<unk>', '<sos>'])"
      ],
      "id": "7TUGyWGuuPSr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b47d735",
        "outputId": "fb77f818-ea9c-4b27-d1ba-ff397741ef19"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'am', 'happy']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "vocabulary.lookup_tokens(indexs)"
      ],
      "id": "2b47d735"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bcb33d1"
      },
      "source": [
        "## 1.2 Dataset & Dataloader"
      ],
      "id": "4bcb33d1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c4a5675"
      },
      "outputs": [],
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer=tokenizer, vocab=vocabulary, video_length=7000, comment_length = 32):\n",
        "        self.video_length = video_length\n",
        "        self.comment_length = comment_length\n",
        "        \n",
        "        self.tokenizer = tokenizer\n",
        "        self.vocab = vocab\n",
        "    \n",
        "        self.video_df = data['video']\n",
        "        self.comment_df = data['comment']\n",
        "\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        vid = self.comment_df.iloc[index]['vid']\n",
        "        \n",
        "        # trg\n",
        "        comment = str(self.comment_df.iloc[index]['en_content'])\n",
        "        tokenized_comment = self.tokenizer(comment)\n",
        "        tagged_comment = self.tag(tokenized_comment, self.comment_length)\n",
        "        comment_idxs = self.vocab.forward(tagged_comment)\n",
        "        \n",
        "        # src\n",
        "        video = self.video_df[self.video_df['vid'] == vid]\n",
        "        \n",
        "        title = str(video['title'].item())\n",
        "        transcript = str(video['transcript'].item())\n",
        "        \n",
        "        video_texts = ' '.join([title, transcript])\n",
        "        \n",
        "        tokenized_video_texts = self.tokenizer(video_texts)[:self.video_length-2]\n",
        "        text_length = self.video_length \n",
        "        \n",
        "        tagged_video_texts = self.tag(tokenized_video_texts, self.video_length)\n",
        "        \n",
        "        video_idxs = self.vocab.forward(tagged_video_texts)\n",
        "        \n",
        "        return torch.tensor(video_idxs), torch.tensor(text_length),torch.tensor(comment_idxs)\n",
        "        # return torch.tensor(video_idxs).to(device), torch.tensor(comment_idxs).to(device)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.comment_df.shape[0]\n",
        "        \n",
        "    def tag(self, words, length):\n",
        "        words.insert(0, '<sos>')\n",
        "        words.append(\"<eos>\")\n",
        "        words = words + ['<pad>']*(length-len(words))\n",
        "        return words\n",
        "        "
      ],
      "id": "7c4a5675"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "805a90a0"
      },
      "outputs": [],
      "source": [
        "def collate_fn(data):\n",
        "    data.sort(key=lambda x: x[1], reverse=True) \n",
        "    video = [row[0].numpy() for row in data]\n",
        "    length = [row[1] for row in data]\n",
        "    comment = [row[2].numpy() for row in data]\n",
        "    # return torch.Tensor(video).int().to(device), torch.Tensor(length).int().to(device), torch.Tensor(comment).int().to(device)\n",
        "    return torch.Tensor(video).int().to(device), torch.Tensor(length), torch.Tensor(comment).int().to(device)"
      ],
      "id": "805a90a0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywGZYg2PxU_p"
      },
      "source": [
        "### Dataset and Dataloader Init"
      ],
      "id": "ywGZYg2PxU_p"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "372ef8fb"
      },
      "outputs": [],
      "source": [
        "train_dataset = TextDataset(train_data)\n",
        "test_dataset = TextDataset(test_data)\n",
        "valid_dataset = TextDataset(valid_data)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, collate_fn=collate_fn)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=16, collate_fn=collate_fn)\n"
      ],
      "id": "372ef8fb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8Dxe2pyxmgs"
      },
      "source": [
        "# 2. Model\n",
        "\n"
      ],
      "id": "Z8Dxe2pyxmgs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0be6cd0f"
      },
      "source": [
        "* `nn.Embedding(num_embedding, embedding_dim)`\n",
        "    * `num_embedding`: vocabulary_size\n",
        "    * `embedding_dim`: embedding vector size -> output size\n",
        "    * output shape : (batch_size, embedding_dim)\n",
        "* `nn.LSTM(input_size, hidden_size, num_layers, batch_first=False)`\n",
        "    * **QUESTION**: what is the `output` of LSTM?\n",
        "        * equal to last hidden of output?\n",
        "* "
      ],
      "id": "0be6cd0f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaA3sr2vxxml"
      },
      "source": [
        "## 2.1 Encoder"
      ],
      "id": "HaA3sr2vxxml"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbd6f4ce"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, embed_dim, hidden_dim, rnn_layers, dropout_ratio):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "        self.rnn = nn.LSTM(embed_dim, hidden_dim, rnn_layers)\n",
        "\n",
        "    def forward(self, src, src_len):\n",
        "        embedded_seq = self.dropout(self.embedding(src))\n",
        "        # embedded_seq [src_len ,batch_size, embedding_dim]\n",
        "\n",
        "        # must work on\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded_seq, src_len.to('cpu'))\n",
        "        # print(packed_embedded.data.shape, packed_embedded.batch_sizes)\n",
        "\n",
        "        # outputs is the final outputs of the hidden states \n",
        "        packed_outputs, (hidden, cell) = self.rnn(packed_embedded)  \n",
        "        # outputs [src_len, batch_size, enc_hidden_dim]\n",
        "        # hidden [n_layers * direction, batch_size, enc_hidden_dim]\n",
        "        # cell [n layers * n directions, batch size, enc_hidden_dim]\n",
        "\n",
        "        outputs, lens_unpacked = nn.utils.rnn.pad_packed_sequence(packed_outputs)\n",
        "\n",
        "        # context vector: the last hidden vector of the encoder\n",
        "        context = outputs[-1, :,:]  # [1, batch_size, enc_hidden_dim]\n",
        "        context = context.squeeze(0)  # [batch_size, enc_hidden_dim]\n",
        "        return outputs, context, hidden, cell"
      ],
      "id": "cbd6f4ce"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAMJFuWKx3eW"
      },
      "source": [
        "## 2.2 Attention"
      ],
      "id": "vAMJFuWKx3eW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92c2e3ed"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attention = nn.Linear(enc_hid_dim + dec_hid_dim, dec_hid_dim)\n",
        "        self.v = nn.Linear(dec_hid_dim, 1, bias=False)  # 输入为hidden_dim, 输出shape = 1\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs, mask):\n",
        "        # hidden = [batch size, dec hid dim]\n",
        "        # encoder_outputs = [src len, batch size, enc hid dim]\n",
        "\n",
        "        batch_size = encoder_outputs.shape[1]\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "\n",
        "        # repeat decoder hidden state src_len times\n",
        "        hidden = hidden.unsqueeze(1)\n",
        "        # hidden shape [batch size, 1, dec hid dim]\n",
        "\n",
        "\n",
        "        hidden = hidden.repeat(1, src_len, 1)  \n",
        "        # hidden = [batch size, src len, dec hid dim] \n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        # encoder_outputs = [batch size, src len, enc hid dim]\n",
        "\n",
        "        concat_attention_input = torch.cat((hidden, encoder_outputs), dim=2)\n",
        "        # [batch size, src len, enc hid dim+dec hid dim]\n",
        "\n",
        "        energy = torch.tanh(self.attention(concat_attention_input))  \n",
        "        # energy = [batch size, src len, dec hid dim]\n",
        "\n",
        "        attention = self.v(energy)\n",
        "        # [batch size, src len, 1]\n",
        "\n",
        "        attention = attention.squeeze(2)\n",
        "        # [batch size, src len]\n",
        "\n",
        "        # mask is True replace with inf\n",
        "        attention = attention.masked_fill(mask, -1e10)\n",
        "\n",
        "        return F.softmax(attention, dim=1)"
      ],
      "id": "92c2e3ed"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jALLLw3rx8l-"
      },
      "source": [
        "## 2.3 Decoder"
      ],
      "id": "jALLLw3rx8l-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95074820"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, embed_dim, hidden_dim, rnn_layers, dropout_ratio, gated_attention):\n",
        "        super().__init__()\n",
        "        # attention added in decoder\n",
        "        self.embedding = nn.Embedding(output_dim, embed_dim)\n",
        "        self.rnn = nn.LSTM(embed_dim, hidden_dim, rnn_layers)\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "        self.gated = gated_attention\n",
        "        self.fcl = nn.Linear(hidden_dim + hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, trg_word, encoder_outputs,hidden=None, cell=None):\n",
        "        '''\n",
        "        trg_word: target word\n",
        "        context: [batch size, dec hid dim]\n",
        "        cell: [n_layers*n_direction, batch_size,enc_dim]\n",
        "        hidden: [n_layers*n_direction, batch_size,enc_dim]\n",
        "        mask: [batch_size, src_len]\n",
        "\n",
        "        '''\n",
        "        trg_seq = trg_word.unsqueeze(0)\n",
        "        # trg_seq [1, batch_size]\n",
        "\n",
        "        embedded = self.dropout(self.embedding(trg_seq))  # embedded comment\n",
        "        # [1, batch_size, embed_dim]\n",
        "\n",
        "        if hidden!= None and cell!=None:\n",
        "            output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        else:\n",
        "            output, (hidden, cell) = self.rnn(embedded)\n",
        "\n",
        "        # output [1, batch_size, dec_hidden_dim]\n",
        "        # hidden [n_layers, batch_size,dec_hidden_dim]\n",
        "        # cell [n_layers, batch_size, dec_hidden_dim]\n",
        "\n",
        "        gated_weight = self.gated(encoder_outputs, output).squeeze(0)\n",
        "        # gated_weight [batch_size, hidden_dim]\n",
        "        output = output.squeeze(0)\n",
        "        # output [batch_size, hidden_dim]\n",
        "\n",
        "        fcl_input = torch.cat((output, gated_weight), dim=1)\n",
        "        # fcl_input [batch_size, hidden_dim+hidden_dim]\n",
        "        prediction = self.fcl(fcl_input)\n",
        "\n",
        "        return prediction, hidden, cell"
      ],
      "id": "95074820"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faWY-Zvqx_rF"
      },
      "source": [
        "## 2.4 Seq2Seq model"
      ],
      "id": "faWY-Zvqx_rF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cec7a88e"
      },
      "outputs": [],
      "source": [
        "class GatedAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.encoder_linear = nn.Linear(hidden_dim, hidden_dim, bias=False) \n",
        "        self.decoder_linear = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
        "        self.attention = nn.Linear(hidden_dim, 1, bias=False)\n",
        "        self.gate = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, encoder_outputs, decoder_outputs):\n",
        "        # encoder_outputs [src_len, batch_size, hidden_dim]\n",
        "        # decoder_outputs [1, batch_size, hidden_dim]\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "        batch_size = encoder_outputs.shape[1]\n",
        "\n",
        "        weighted_h = self.encoder_linear(encoder_outputs)\n",
        "        weighted_s = self.decoder_linear(decoder_outputs)\n",
        "        # weighted_h [src_len, batch_size, hidden_dim]\n",
        "        # weighted_s [1, batch_size, hidden_dim]\n",
        "\n",
        "        weighted_s = weighted_s.repeat(src_len, 1, 1)\n",
        "        # weighted_s [src_len, batch_size, hidden_dim]\n",
        "\n",
        "        weighted = torch.tanh(weighted_h + weighted_s)\n",
        "        # weighted [src_len, batch_size, hidden_dim]\n",
        "\n",
        "        a = self.attention(weighted)\n",
        "        # attention [src_len, batch_size, 1]\n",
        "        a = F.softmax(a, dim=0)\n",
        "\n",
        "        a = a.permute(1, 2, 0)\n",
        "        # a [batch_size, 1, src_len]\n",
        "        encoder_outputs = encoder_outputs.permute(1,0,2)\n",
        "        # encoder_outputs [batch_size, src_len, hidden_dim]\n",
        "\n",
        "        context = torch.bmm(a, encoder_outputs)\n",
        "        # context [batch_size, 1, hidden_dim]\n",
        "        context = context.permute(1,0,2)\n",
        "        # context [1, batch_size, hidden_dim]\n",
        "\n",
        "        m = self.gate(decoder_outputs)\n",
        "        # m [1, batch_size, hidden_dim]\n",
        "        m = torch.sigmoid(m)\n",
        "\n",
        "        gated_weight = m.mul(context)\n",
        "        # gated_weight [1, batch_size, hidden_dim]\n",
        "\n",
        "        return gated_weight"
      ],
      "id": "cec7a88e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2Ohycp0JFuB"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, attention, vocab, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.attention = attention\n",
        "        self.vocab = vocab\n",
        "        self.pad_idx = vocab['<pad>']\n",
        "        self.device = device\n",
        "\n",
        "    def create_mask(self, src):\n",
        "        mask = src != self.pad_idx\n",
        "        mask = mask.permute(1, 0)\n",
        "        # mask [batch_size, src_len]\n",
        "        return mask\n",
        "\n",
        "    def forward(self, src, src_len, trg, teacher_forcing_ratio=0.5):\n",
        "        # src = [src len, batch size]\n",
        "        # src_len = [batch size]\n",
        "        # trg = [trg len, batch size]\n",
        "\n",
        "        trg_len = trg.shape[0]\n",
        "        batch_size = trg.shape[1]\n",
        "        vocab_size = len(self.vocab)\n",
        "\n",
        "        # init record decoder outputs\n",
        "        decoder_outputs = torch.zeros(trg_len, batch_size, vocab_size).to(self.device)\n",
        "\n",
        "        encoder_outputs, encoder_context, hidden, cell = self.encoder(src, src_len)\n",
        "        # encoder_outputs [src_len, batch_size, enc_hidden_dim]\n",
        "        # encoder_context [batch_size, enc_hidden_dim]\n",
        "\n",
        "        # the first of the target_word is <sos>\n",
        "        target_word = trg[0, :]\n",
        "        # pad attention\n",
        "        mask = self.create_mask(src)\n",
        "        a = self.attention(encoder_context, encoder_outputs, mask)\n",
        "        # a [batch_size, src_len]\n",
        "        a = a.unsqueeze(1)\n",
        "        # a [batch_size, 1, src_len]\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        # encoder_outputs [batch_size, src_len, enc_hidden_dim]\n",
        "\n",
        "        weighted_encoder_outputs = torch.bmm(a, encoder_outputs) \n",
        "        # weighted [batch_size, 1, enc_hidden_dim]\n",
        "        weighted_encoder_outputs = weighted_encoder_outputs.permute(1, 0, 2)\n",
        "        # weighted [1, batch_size, enc_hidden_dim]\n",
        "\n",
        "        for t in range(1, trg_len):\n",
        "            outputs, hidden, cell = self.decoder(target_word,weighted_encoder_outputs, hidden, cell)\n",
        "\n",
        "            # output: Probability distribution\n",
        "            decoder_outputs[t] = outputs\n",
        "\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "\n",
        "            top1_word = outputs.argmax(1)\n",
        "            target_word = trg[t] if teacher_force else top1_word\n",
        "\n",
        "        return decoder_outputs"
      ],
      "id": "L2Ohycp0JFuB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b764746"
      },
      "source": [
        "## 2.5 Train & Evaulation"
      ],
      "id": "0b764746"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f5a74ac"
      },
      "outputs": [],
      "source": [
        "# from torchtext.data.metrics import bleu_score\n",
        "def train(model, loader, optimizer, clip, loss_fn=nn.CrossEntropyLoss()):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for video_text, text_len, comment in tqdm(loader):\n",
        "        optimizer.zero_grad()\n",
        "        video_text = video_text.permute(1,0)\n",
        "        comment = comment.permute(1,0) # [src len, batch size]\n",
        "        \n",
        "        output = model(video_text,text_len, comment)\n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        comment = comment[1:].contiguous().long().view(-1)\n",
        "        \n",
        "        loss = loss_fn(output, comment)\n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    return epoch_loss / len(loader)"
      ],
      "id": "2f5a74ac"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aaaf370"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, loader,loss_fn=nn.CrossEntropyLoss()):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for video_text, text_length, comment in tqdm(loader): \n",
        "            \n",
        "            video_text = video_text.permute(1,0)\n",
        "            comment = comment.permute(1,0)\n",
        "        \n",
        "            output = model(video_text, text_length, comment)\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "\n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            comment = comment[1:].contiguous().long().view(-1)\n",
        "\n",
        "            loss = loss_fn(output, comment)\n",
        "            epoch_loss += loss.item()\n",
        "    return epoch_loss / len(loader)"
      ],
      "id": "1aaaf370"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3ab7620"
      },
      "outputs": [],
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "id": "b3ab7620"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4z4OwVcyHMl"
      },
      "source": [
        "# 3. Train"
      ],
      "id": "v4z4OwVcyHMl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8f73723d"
      },
      "outputs": [],
      "source": [
        "# Model Parameters\n",
        "INPUT_DIM = len(vocabulary)\n",
        "OUTPUT_DIM = len(vocabulary)\n",
        "\n",
        "ENCODER_EMBEDDED_DIM = 256\n",
        "DECODER_EMBEDDED_DIM = 256\n",
        "HIDDEN_DIM = 512\n",
        "NUM_LAYERS = 2\n",
        "ENCODER_DROPOUT = 0.5\n",
        "DECODER_DROPOUT = 0.5\n",
        "\n",
        "encoder = Encoder(INPUT_DIM, ENCODER_EMBEDDED_DIM, HIDDEN_DIM, NUM_LAYERS,ENCODER_DROPOUT).to(device)\n",
        "attention = Attention(HIDDEN_DIM, HIDDEN_DIM).to(device)\n",
        "gated_attention = GatedAttention(HIDDEN_DIM).to(device)\n",
        "decoder = Decoder(OUTPUT_DIM, DECODER_EMBEDDED_DIM, HIDDEN_DIM, NUM_LAYERS, DECODER_DROPOUT,gated_attention).to(device)\n",
        "\n",
        "model = Seq2Seq(encoder, decoder, attention,vocabulary, device).to(device)"
      ],
      "id": "8f73723d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dsVa-gNTuG4",
        "outputId": "5aaad3ba-75d6-4025-8e73-016554b903c5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(50437, 256)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (rnn): LSTM(256, 512, num_layers=2)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(50437, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (gated): GatedAttention(\n",
              "      (encoder_linear): Linear(in_features=512, out_features=512, bias=False)\n",
              "      (decoder_linear): Linear(in_features=512, out_features=512, bias=False)\n",
              "      (attention): Linear(in_features=512, out_features=1, bias=False)\n",
              "      (gate): Linear(in_features=512, out_features=512, bias=True)\n",
              "    )\n",
              "    (fcl): Linear(in_features=1024, out_features=50437, bias=True)\n",
              "  )\n",
              "  (attention): Attention(\n",
              "    (attention): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (v): Linear(in_features=512, out_features=1, bias=False)\n",
              "  )\n",
              "  (vocab): Vocab()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "        \n",
        "model.apply(init_weights)"
      ],
      "id": "7dsVa-gNTuG4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__tYQSvV0POt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7796f4e-8a54-476c-a3f9-8bbb50beea0e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# model_path = os.path.join(WORK_DIR, 'Models','Pad+GateModel4')\n",
        "# results = torch.load(os.path.join(model_path, 'model.pt'))\n",
        "# best_valid_loss = results['valid_loss']\n",
        "# model.load_state_dict(results['state_dict'])"
      ],
      "id": "__tYQSvV0POt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7256043",
        "outputId": "bc54ed3d-4dbc-47f0-a82f-b4c9a0c003c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/4272 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  import sys\n",
            "100%|██████████| 4272/4272 [2:30:13<00:00,  2.11s/it]\n",
            "100%|██████████| 1240/1240 [11:30<00:00,  1.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 07 | Time: 161m 44s\n",
            "\tTrain Loss: 2.642 | Train PPL:  14.048\n",
            "\t Val. Loss: 2.946 |  Val. PPL:  19.038\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4272/4272 [2:30:16<00:00,  2.11s/it]\n",
            "100%|██████████| 1240/1240 [11:31<00:00,  1.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 08 | Time: 161m 47s\n",
            "\tTrain Loss: 2.528 | Train PPL:  12.523\n",
            "\t Val. Loss: 2.965 |  Val. PPL:  19.392\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4272/4272 [2:30:08<00:00,  2.11s/it]\n",
            "100%|██████████| 1240/1240 [11:33<00:00,  1.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 09 | Time: 161m 41s\n",
            "\tTrain Loss: 2.446 | Train PPL:  11.538\n",
            "\t Val. Loss: 2.990 |  Val. PPL:  19.888\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4272/4272 [2:30:21<00:00,  2.11s/it]\n",
            "100%|██████████| 1240/1240 [11:34<00:00,  1.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 10 | Time: 161m 55s\n",
            "\tTrain Loss: 2.379 | Train PPL:  10.791\n",
            "\t Val. Loss: 3.033 |  Val. PPL:  20.756\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 10\n",
        "CLIP = 1\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "\n",
        "for epoch in range( EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(model, train_loader, optimizer= optimizer, clip=CLIP)\n",
        "    valid_loss = evaluate(model, valid_loader)\n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    result_dict = {\n",
        "        'epoch':epoch + 1,\n",
        "        'train_loss':train_loss,\n",
        "        'valid_loss':valid_loss,\n",
        "        'train_ppl':math.exp(train_loss),\n",
        "        'valid_ppl':math.exp(valid_loss),\n",
        "        'state_dict': model.state_dict(),\n",
        "    }\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        torch.save(result_dict, os.path.join(model_path, f'model.pt'))\n",
        "        best_valid_loss = valid_loss\n",
        "    \n",
        "    with open(os.path.join(model_path, 'evluation-results.csv'), 'a')as f:\n",
        "        f.write(','.join([str(item) for item in list(result_dict.values())[:-1]]))\n",
        "        f.write('\\n')\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {result_dict[\"train_ppl\"]:7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {result_dict[\"valid_ppl\"]:7.3f}')"
      ],
      "id": "c7256043"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnnFxqlzjwQv"
      },
      "source": [
        "# 4. Evaluation"
      ],
      "id": "hnnFxqlzjwQv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "156TksOeC8ES"
      },
      "source": [
        "## TestDataset"
      ],
      "id": "156TksOeC8ES"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8GseT0gC7CK"
      },
      "outputs": [],
      "source": [
        "class TestTextDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer=tokenizer, vocab=vocabulary, video_length=7000, comment_length = 32):\n",
        "        self.video_length = video_length\n",
        "        self.comment_length = comment_length\n",
        "        \n",
        "        self.tokenizer = tokenizer\n",
        "        self.vocab = vocab\n",
        "    \n",
        "        self.video_df = data['video']\n",
        "        self.comment_df = data['comment']\n",
        "\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        vid = self.comment_df.iloc[index]['vid']\n",
        "        \n",
        "        # trg\n",
        "        comment = str(self.comment_df.iloc[index]['en_content'])\n",
        "        tokenized_comment = self.tokenizer(comment)\n",
        "        tagged_comment = self.tag(tokenized_comment, self.comment_length)\n",
        "        comment_idxs = self.vocab.forward(tagged_comment)\n",
        "        \n",
        "        # src\n",
        "        video = self.video_df[self.video_df['vid'] == vid]\n",
        "        \n",
        "        title = str(video['title'].item())\n",
        "        transcript = str(video['transcript'].item())\n",
        "        \n",
        "        video_texts = ' '.join([title, transcript])\n",
        "        \n",
        "        tokenized_video_texts = self.tokenizer(video_texts)[:self.video_length-2]\n",
        "        text_length = self.video_length \n",
        "        \n",
        "        tagged_video_texts = self.tag(tokenized_video_texts, self.video_length)\n",
        "        \n",
        "        video_idxs = self.vocab.forward(tagged_video_texts)\n",
        "        \n",
        "        return torch.tensor(video_idxs), torch.tensor(text_length),torch.tensor(comment_idxs), vid\n",
        "        # return torch.tensor(video_idxs).to(device), torch.tensor(comment_idxs).to(device)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.comment_df.shape[0]\n",
        "        \n",
        "    def tag(self, words, length):\n",
        "        words.insert(0, '<sos>')\n",
        "        words.append(\"<eos>\")\n",
        "        words = words + ['<pad>']*(length-len(words))\n",
        "        return words\n",
        "        "
      ],
      "id": "t8GseT0gC7CK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpFH4SmCj1D3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "598457f6-6139-4f4e-8bb2-439949d5135a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "# model_path = os.path.join(WORK_DIR, 'Models','Pad+GateModel2', 'epoch8.pt')\n",
        "model_path = os.path.join(WORK_DIR, 'Models','Pad+GateModel4', 'model-10.pt')\n",
        "result = torch.load(model_path)\n",
        "model.load_state_dict(result['state_dict'])"
      ],
      "id": "gpFH4SmCj1D3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPpaCBMNEurP"
      },
      "outputs": [],
      "source": [
        "def test_collate_fn(data):\n",
        "    data.sort(key=lambda x: x[1], reverse=True) \n",
        "    video = [row[0].numpy() for row in data]\n",
        "    length = [row[1] for row in data]\n",
        "    comment = [row[2].numpy() for row in data]\n",
        "    vid = [row[3]for row in data]\n",
        "    # return torch.Tensor(video).int().to(device), torch.Tensor(length).int().to(device), torch.Tensor(comment).int().to(device)\n",
        "    return torch.Tensor(video).int().to(device), torch.Tensor(length), torch.Tensor(comment).int().to(device), vid"
      ],
      "id": "lPpaCBMNEurP"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate comments on Test dataset and Save the result"
      ],
      "metadata": {
        "id": "o83vGzLySU0s"
      },
      "id": "o83vGzLySU0s"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwT4zxgQmkh9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "efc13aec-9a45-4a0c-af2d-ea107db6652a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/304 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  \n",
            "100%|██████████| 304/304 [12:06<00:00,  2.39s/it]\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "test_dataset = TestTextDataset(test_data)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, collate_fn=test_collate_fn)\n",
        "ratio = 0.5\n",
        "with torch.no_grad():\n",
        "    for video_texts, text_lens, comments, vid in tqdm(test_loader): \n",
        "        results = []\n",
        "        video_texts = video_texts.permute(1,0)\n",
        "        comments = comments.permute(1,0)\n",
        "        outputs = model(video_texts,text_lens, comments, ratio)\n",
        "        outputs = outputs.permute(1, 0 ,2)\n",
        "        comments = comments.permute(1, 0)\n",
        "        \n",
        "        for i in range(outputs.shape[0]):\n",
        "            generated_comment_index = outputs[i,:,:].argmax(1)\n",
        "            generated_comment = vocabulary.lookup_tokens(list(generated_comment_index))\n",
        "            generated_comment = ' '.join([word for word in generated_comment if word not in ['<pad>', '<eos>', '<sos>']])\n",
        "            comment = vocabulary.lookup_tokens(list(comments[i]))\n",
        "            comment = ' '.join([word for word in comment if word not in ['<pad>', '<eos>', '<sos>']])\n",
        "            results.append(['PadGateModel4',vid[i], str(ratio), comment, generated_comment, '\\n']) \n",
        "        with open(WORK_DIR+'Models/Pad+GateModel4/'+'predictions-10.csv', 'a') as f:\n",
        "            f.writelines([','.join(line) for line in results])"
      ],
      "id": "jwT4zxgQmkh9"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "[MscProject]GatedAttentionModel.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python [conda env:deeplearning]",
      "language": "python",
      "name": "conda-env-deeplearning-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}