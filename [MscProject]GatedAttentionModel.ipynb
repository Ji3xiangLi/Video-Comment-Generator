{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Telescope-U/Video-Comment-Generator/blob/GateModel/%5BMscProject%5DGatedAttentionModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89805564"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchtext\n",
        "from torchtext.vocab import build_vocab_from_iterator\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import math\n",
        "import time\n",
        "import os\n",
        "import random\n",
        "from tqdm import tqdm"
      ],
      "id": "89805564"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3yyKyXwYG2Y",
        "outputId": "6faeb5f3-010b-4210-e83c-78c2e1b51171"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "id": "A3yyKyXwYG2Y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fKfgl4DWVrae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae2910fd-dc71-40ae-82fa-01397511d22a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "fKfgl4DWVrae"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "853e789b"
      },
      "outputs": [],
      "source": [
        "WORK_DIR = '/content/drive/MyDrive/Colab Notebooks/[Msc]Video-Comment-Generator/'\n",
        "if not os.path.isdir(WORK_DIR):\n",
        "    WORK_DIR = ''\n",
        "TRAIN_FOLDER = WORK_DIR + 'Dataset/Train/'\n",
        "VALID_FOLDER = WORK_DIR + 'Dataset/Valid/'\n",
        "TEST_FOLDER = WORK_DIR + 'Dataset/Test/'\n",
        "VIDEO_PATH = WORK_DIR + 'Dataset/video_data.csv'\n",
        "COMMENT_PATH = WORK_DIR + 'Dataset/comment_data.csv'"
      ],
      "id": "853e789b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a90612b"
      },
      "source": [
        "# 1. Dataset"
      ],
      "id": "7a90612b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "288bbd13"
      },
      "outputs": [],
      "source": [
        "# 数据分离部分\n",
        "# def split_ids(video_ids, train=0.7, valid=0.1, test=0.2):\n",
        "#     list_copy = video_ids.copy()\n",
        "#     random.shuffle(list_copy)\n",
        "#     train_size = math.floor(len(list_copy)*train)\n",
        "#     valid_size = math.floor(len(list_copy)*valid)\n",
        "#     return list_copy[:train_size], list_copy[train_size:(train_size+valid_size)], list_copy[(train_size+valid_size):]"
      ],
      "id": "288bbd13"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "002ab143"
      },
      "outputs": [],
      "source": [
        "# video_df = pd.read_csv(VIDEO_PATH)\n",
        "# comment_df = pd.read_csv(COMMENT_PATH)\n",
        "# train_ids, valid_ids, test_ids = split_ids(video_df['vid'].unique())\n",
        "# print(len(train_ids), len(valid_ids), len(test_ids))"
      ],
      "id": "002ab143"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fd3f1bff"
      },
      "outputs": [],
      "source": [
        "# import string\n",
        "# def clean_text(text):\n",
        "#     text = str(text)\n",
        "#     for i in string.punctuation:\n",
        "#         text = text.replace(i, '')\n",
        "#     return text.lower()\n",
        "\n",
        "# video_df['title'] = video_df['title'].apply(clean_text)\n",
        "# video_df['transcript'] = video_df['transcript'].apply(clean_text)\n",
        "# comment_df['en_content'] = comment_df['en_content'].apply(clean_text)"
      ],
      "id": "fd3f1bff"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "751dc8d5"
      },
      "outputs": [],
      "source": [
        "# # 分离训练数据并且存储数据\n",
        "# train_video_df = video_df[video_df['vid'].isin(train_ids)]\n",
        "# valid_video_df = video_df[video_df['vid'].isin(valid_ids)]\n",
        "# test_video_df = video_df[video_df['vid'].isin(test_ids)]\n",
        "\n",
        "# train_comment_df = comment_df[comment_df['vid'].isin(train_ids)]\n",
        "# valid_comment_df = comment_df[comment_df['vid'].isin(valid_ids)]\n",
        "# test_comment_df = comment_df[comment_df['vid'].isin(test_ids)]\n",
        "\n",
        "# train_comment_df.to_csv('Dataset/Train/comment.csv', index = None)\n",
        "# test_comment_df.to_csv('Dataset/Test/comment.csv', index = None)\n",
        "# valid_comment_df.to_csv(\"Dataset/Valid/comment.csv\", index = None)\n",
        "\n",
        "# train_video_df.to_csv('Dataset/Train/video.csv', index = None)\n",
        "# test_video_df.to_csv('Dataset/Test/video.csv', index = None)\n",
        "# valid_video_df.to_csv(\"Dataset/Valid/video.csv\", index = None)"
      ],
      "id": "751dc8d5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3510497a"
      },
      "outputs": [],
      "source": [
        "def get_data(folder):\n",
        "    comment_path = os.path.join(folder,'comment.csv')\n",
        "    video_path = os.path.join(folder,'video.csv')\n",
        "    comment_df = pd.read_csv(comment_path)\n",
        "    video_df = pd.read_csv(video_path)\n",
        "    return {'comment':comment_df, 'video': video_df}\n",
        "        "
      ],
      "id": "3510497a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "374e4e20"
      },
      "outputs": [],
      "source": [
        "train_data = get_data(TRAIN_FOLDER)\n",
        "valid_data = get_data(VALID_FOLDER)\n",
        "test_data = get_data(TEST_FOLDER)"
      ],
      "id": "374e4e20"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3326e42"
      },
      "source": [
        "## 1.1 Vocabulary"
      ],
      "id": "e3326e42"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "390e1ce4"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "specials = ['<unk>','<pad>', '<sos>', '<eos>']\n",
        "\n",
        "word_list = []\n",
        "\n",
        "def yield_tokens():\n",
        "    for data in [train_data, valid_data]:\n",
        "        columns = [data['comment']['en_content'],\n",
        "                data['video']['title'],\n",
        "                data['video']['transcript']]\n",
        "\n",
        "        token_lists = [tokenizer(str(text)) for column in columns for text in column]\n",
        "        for tokens in token_lists:\n",
        "            yield tokens\n",
        "        \n",
        "vocabulary = build_vocab_from_iterator(yield_tokens(), specials=specials, min_freq=2)\n",
        "vocabulary.set_default_index(vocabulary['<unk>'])"
      ],
      "id": "390e1ce4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ada0bfcb",
        "outputId": "50ae32de-f7e5-4939-be92-f214c4c4ac70",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50437"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "len(vocabulary)"
      ],
      "id": "ada0bfcb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "516e9085",
        "outputId": "27a8e6c3-6192-4458-db66-52bad5ae5dbf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[6, 166, 198]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "sentence = \"i am happy\".split()\n",
        "indexs = vocabulary.forward(sentence)\n",
        "indexs"
      ],
      "id": "516e9085"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7TUGyWGuuPSr",
        "outputId": "447c7c9c-33ce-47d3-f66f-6c5fbad4668b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3, 0, 2]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "vocabulary.forward(['<eos>', '<unk>', '<sos>'])"
      ],
      "id": "7TUGyWGuuPSr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b47d735",
        "outputId": "f2b1ba11-c69e-4c2e-94ba-126247ed1581"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i', 'am', 'happy']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "vocabulary.lookup_tokens(indexs)"
      ],
      "id": "2b47d735"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bcb33d1"
      },
      "source": [
        "## 1.2 Dataset & Dataloader"
      ],
      "id": "4bcb33d1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c4a5675"
      },
      "outputs": [],
      "source": [
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer=tokenizer, vocab=vocabulary, video_length=7000, comment_length = 32):\n",
        "        self.video_length = video_length\n",
        "        self.comment_length = comment_length\n",
        "        \n",
        "        self.tokenizer = tokenizer\n",
        "        self.vocab = vocab\n",
        "    \n",
        "        self.video_df = data['video']\n",
        "        self.comment_df = data['comment']\n",
        "\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        vid = self.comment_df.iloc[index]['vid']\n",
        "        \n",
        "        # trg\n",
        "        comment = str(self.comment_df.iloc[index]['en_content'])\n",
        "        tokenized_comment = self.tokenizer(comment)\n",
        "        tagged_comment = self.tag(tokenized_comment, self.comment_length)\n",
        "        comment_idxs = self.vocab.forward(tagged_comment)\n",
        "        \n",
        "        # src\n",
        "        video = self.video_df[self.video_df['vid'] == vid]\n",
        "        \n",
        "        title = str(video['title'].item())\n",
        "        transcript = str(video['transcript'].item())\n",
        "        \n",
        "        video_texts = ' '.join([title, transcript])\n",
        "        \n",
        "        tokenized_video_texts = self.tokenizer(video_texts)[:self.video_length-2]\n",
        "        text_length = self.video_length \n",
        "        \n",
        "        tagged_video_texts = self.tag(tokenized_video_texts, self.video_length)\n",
        "        \n",
        "        video_idxs = self.vocab.forward(tagged_video_texts)\n",
        "        \n",
        "        return torch.tensor(video_idxs), torch.tensor(text_length),torch.tensor(comment_idxs)\n",
        "        # return torch.tensor(video_idxs).to(device), torch.tensor(comment_idxs).to(device)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.comment_df.shape[0]\n",
        "        \n",
        "    def tag(self, words, length):\n",
        "        words.insert(0, '<sos>')\n",
        "        words.append(\"<eos>\")\n",
        "        words = words + ['<pad>']*(length-len(words))\n",
        "        return words\n",
        "        "
      ],
      "id": "7c4a5675"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "805a90a0"
      },
      "outputs": [],
      "source": [
        "def collate_fn(data):\n",
        "    data.sort(key=lambda x: x[1], reverse=True) \n",
        "    video = [row[0].numpy() for row in data]\n",
        "    length = [row[1] for row in data]\n",
        "    comment = [row[2].numpy() for row in data]\n",
        "    # return torch.Tensor(video).int().to(device), torch.Tensor(length).int().to(device), torch.Tensor(comment).int().to(device)\n",
        "    return torch.Tensor(video).int().to(device), torch.Tensor(length), torch.Tensor(comment).int().to(device)"
      ],
      "id": "805a90a0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywGZYg2PxU_p"
      },
      "source": [
        "### Dataset 和 Dataloader 实例化"
      ],
      "id": "ywGZYg2PxU_p"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "372ef8fb"
      },
      "outputs": [],
      "source": [
        "train_dataset = TextDataset(train_data)\n",
        "test_dataset = TextDataset(test_data)\n",
        "valid_dataset = TextDataset(valid_data)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
        "test_loader = DataLoader(test_dataset, batch_size=16, collate_fn=collate_fn)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=16, collate_fn=collate_fn)\n"
      ],
      "id": "372ef8fb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8Dxe2pyxmgs"
      },
      "source": [
        "# 2. Model\n",
        "\n"
      ],
      "id": "Z8Dxe2pyxmgs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0be6cd0f"
      },
      "source": [
        "* `nn.Embedding(num_embedding, embedding_dim)`\n",
        "    * `num_embedding`: vocabulary_size\n",
        "    * `embedding_dim`: embedding vector size -> 输出维度\n",
        "    * output shape : (batch_size, embedding_dim)\n",
        "* `nn.LSTM(input_size, hidden_size, num_layers, batch_first=False)`\n",
        "    * **QUESTION**: what is the `output` of LSTM?\n",
        "        * equal to last hidden of output?\n",
        "* "
      ],
      "id": "0be6cd0f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaA3sr2vxxml"
      },
      "source": [
        "## 2.1 Encoder"
      ],
      "id": "HaA3sr2vxxml"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cbd6f4ce"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    # Q: 如果要按照weibo模型进行修改的话，是不是encoder 需要改成 seq2seq class中 decoder的样子，for循环\n",
        "    # A: 不用，因为encoder outputs就是所有的时间步的最末层hidden_state 即 H = [h_1, h_2, ..., h_T]\n",
        "\n",
        "    def __init__(self, input_dim, embed_dim, hidden_dim, rnn_layers, dropout_ratio):\n",
        "        super().__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "\n",
        "        self.rnn = nn.LSTM(embed_dim, hidden_dim, rnn_layers)\n",
        "\n",
        "    def forward(self, src, src_len):\n",
        "        embedded_seq = self.dropout(self.embedding(src))\n",
        "        # embedded_seq [src_len ,batch_size, embedding_dim]\n",
        "\n",
        "        #!!!这一步必须在cpu上进行\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded_seq, src_len.to('cpu'))\n",
        "        # print(packed_embedded.data.shape, packed_embedded.batch_sizes)\n",
        "\n",
        "        # outputs 是所有hidden states hidden 是最终输出\n",
        "        packed_outputs, (hidden, cell) = self.rnn(packed_embedded)  # 和教程不同，教程中使用gru 只有outputs 和 hidden\n",
        "        # outputs [src_len, batch_size, enc_hidden_dim]\n",
        "        # hidden [n_layers * direction, batch_size, enc_hidden_dim]\n",
        "        # cell [n layers * n directions, batch size, enc_hidden_dim]\n",
        "\n",
        "        # 将outputs 解压,lens 是实际有意义的部分\n",
        "        outputs, lens_unpacked = nn.utils.rnn.pad_packed_sequence(packed_outputs)\n",
        "\n",
        "        # context vector\n",
        "        # 根据news的意思，将h_T（最后一个hidden 作为语义向量）\n",
        "        context = outputs[-1, :,:]  # [1, batch_size, enc_hidden_dim]\n",
        "        context = context.squeeze(0)  # [batch_size, enc_hidden_dim]\n",
        "        return outputs, context, hidden, cell"
      ],
      "id": "cbd6f4ce"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34691819"
      },
      "source": [
        "demo 中有很多处理数据形状的，其实按需求加维减维就好了 不需要特别头疼"
      ],
      "id": "34691819"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAMJFuWKx3eW"
      },
      "source": [
        "## 2.2 Attention"
      ],
      "id": "vAMJFuWKx3eW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92c2e3ed"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, enc_hid_dim, dec_hid_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.attention = nn.Linear(enc_hid_dim + dec_hid_dim, dec_hid_dim)\n",
        "        self.v = nn.Linear(dec_hid_dim, 1, bias=False)  # 输入为hidden_dim, 输出shape = 1\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs, mask):\n",
        "        # hidden = [batch size, dec hid dim]\n",
        "        # encoder_outputs = [src len, batch size, enc hid dim]\n",
        "\n",
        "        batch_size = encoder_outputs.shape[1]\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "\n",
        "        # repeat decoder hidden state src_len times\n",
        "        hidden = hidden.unsqueeze(1)\n",
        "        # hidden shape [batch size, 1, dec hid dim]\n",
        "\n",
        "\n",
        "        hidden = hidden.repeat(1, src_len, 1)  # repeat() 最终结果就是（1, src len, 1）次\n",
        "        # hidden = [batch size, src len, dec hid dim] (可以理解为shape 元素相乘了)\n",
        "\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        # encoder_outputs = [batch size, src len, enc hid dim]\n",
        "\n",
        "        concat_attention_input = torch.cat((hidden, encoder_outputs), dim=2)\n",
        "        # [batch size, src len, enc hid dim+dec hid dim]\n",
        "\n",
        "        energy = torch.tanh(self.attention(concat_attention_input))  # dim 就是拼接的维度\n",
        "        # energy = [batch size, src len, dec hid dim]\n",
        "\n",
        "        attention = self.v(energy)\n",
        "        # [batch size, src len, 1]\n",
        "\n",
        "        attention = attention.squeeze(2)\n",
        "        # [batch size, src len]\n",
        "\n",
        "        # mask is True 就用极小值替代\n",
        "        attention = attention.masked_fill(mask, -1e10)\n",
        "\n",
        "        return F.softmax(attention, dim=1)"
      ],
      "id": "92c2e3ed"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jALLLw3rx8l-"
      },
      "source": [
        "## 2.3 Decoder"
      ],
      "id": "jALLLw3rx8l-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95074820"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, embed_dim, hidden_dim, rnn_layers, dropout_ratio, gated_attention):\n",
        "        super().__init__()\n",
        "        # attention 机制加在decoder 中\n",
        "        self.embedding = nn.Embedding(output_dim, embed_dim)\n",
        "        self.rnn = nn.LSTM(embed_dim, hidden_dim, rnn_layers)\n",
        "        self.dropout = nn.Dropout(dropout_ratio)\n",
        "        self.gated = gated_attention\n",
        "        self.fcl = nn.Linear(hidden_dim + hidden_dim, output_dim)\n",
        "    # 因为attention 的需要添加了encoder_outputs\n",
        "    # def forward(self, trg_word, encoder_outputs, context, hidden, cell, mask):\n",
        "    def forward(self, trg_word, encoder_outputs,hidden=None, cell=None):\n",
        "        '''\n",
        "        trg_word: 目标词汇 是一个单独的词\n",
        "        context: [batch size, dec hid dim]\n",
        "        cell: [n_layers*n_direction, batch_size,enc_dim]\n",
        "        hidden: [n_layers*n_direction, batch_size,enc_dim]\n",
        "        mask: [batch_size, src_len]\n",
        "\n",
        "        '''\n",
        "        # 因为不是序列，所以需要加一步，[1, batch_size]\n",
        "        trg_seq = trg_word.unsqueeze(0)\n",
        "        # trg_seq [1, batch_size]\n",
        "\n",
        "        # 可视做 seq_len = 1\n",
        "        embedded = self.dropout(self.embedding(trg_seq))  # embedded comment\n",
        "        # [1, batch_size, embed_dim]\n",
        "\n",
        "        # hidden和cell初始值为encoder 的最终状态 #4中没有cell，不确定cell要不要压缩\n",
        "        if hidden!= None and cell!=None:\n",
        "            output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        else:\n",
        "            output, (hidden, cell) = self.rnn(embedded)\n",
        "\n",
        "        # output [1, batch_size, dec_hidden_dim]\n",
        "        # hidden [n_layers, batch_size,dec_hidden_dim]\n",
        "        # cell [n_layers, batch_size, dec_hidden_dim]\n",
        "\n",
        "        gated_weight = self.gated(encoder_outputs, output).squeeze(0)\n",
        "        # gated_weight [batch_size, hidden_dim]\n",
        "        output = output.squeeze(0)\n",
        "        # output [batch_size, hidden_dim]\n",
        "\n",
        "        fcl_input = torch.cat((output, gated_weight), dim=1)\n",
        "        # fcl_input [batch_size, hidden_dim+hidden_dim]\n",
        "        prediction = self.fcl(fcl_input)\n",
        "\n",
        "        return prediction, hidden, cell"
      ],
      "id": "95074820"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faWY-Zvqx_rF"
      },
      "source": [
        "## 2.4 Seq2Seq model"
      ],
      "id": "faWY-Zvqx_rF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cec7a88e"
      },
      "outputs": [],
      "source": [
        "class GatedAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.encoder_linear = nn.Linear(hidden_dim, hidden_dim, bias=False) \n",
        "        self.decoder_linear = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
        "        self.attention = nn.Linear(hidden_dim, 1, bias=False)\n",
        "        self.gate = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, encoder_outputs, decoder_outputs):\n",
        "        # encoder_outputs [src_len, batch_size, hidden_dim]\n",
        "        # decoder_outputs [1, batch_size, hidden_dim]\n",
        "        src_len = encoder_outputs.shape[0]\n",
        "        batch_size = encoder_outputs.shape[1]\n",
        "\n",
        "        weighted_h = self.encoder_linear(encoder_outputs)\n",
        "        weighted_s = self.decoder_linear(decoder_outputs)\n",
        "        # weighted_h [src_len, batch_size, hidden_dim]\n",
        "        # weighted_s [1, batch_size, hidden_dim]\n",
        "\n",
        "        weighted_s = weighted_s.repeat(src_len, 1, 1)\n",
        "        # weighted_s [src_len, batch_size, hidden_dim]\n",
        "\n",
        "        weighted = torch.tanh(weighted_h + weighted_s)\n",
        "        # weighted [src_len, batch_size, hidden_dim]\n",
        "\n",
        "        score = self.attention(weighted)\n",
        "        # attention [src_len, batch_size, 1]\n",
        "        score = F.softmax(score, dim=0) # 正则化\n",
        "\n",
        "        score = score.permute(1, 2, 0)\n",
        "        # score [batch_size, 1, src_len]\n",
        "        encoder_outputs = encoder_outputs.permute(1,0,2)\n",
        "        # encoder_outputs [batch_size, src_len, hidden_dim]\n",
        "\n",
        "        context = torch.bmm(score, encoder_outputs)\n",
        "        # context [batch_size, 1, hidden_dim] src_len应该为1？\n",
        "        context = context.permute(1,0,2)\n",
        "        # context [1, batch_size, hidden_dim] src_len应该为1？\n",
        "\n",
        "        m = self.gate(decoder_outputs)\n",
        "        # m [1, batch_size, hidden_dim]\n",
        "        m = torch.sigmoid(m)\n",
        "\n",
        "        gated_weight = m.mul(context)\n",
        "        # gated_weight [1, batch_size, hidden_dim]\n",
        "\n",
        "        return gated_weight"
      ],
      "id": "cec7a88e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2Ohycp0JFuB"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, attention, vocab, device):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.attention = attention\n",
        "        self.vocab = vocab\n",
        "        self.pad_idx = vocab['<pad>']\n",
        "        self.device = device\n",
        "\n",
        "    def create_mask(self, src):\n",
        "        mask = src != self.pad_idx\n",
        "        mask = mask.permute(1, 0)\n",
        "        # mask [batch_size, src_len]\n",
        "        return mask\n",
        "\n",
        "    def forward(self, src, src_len, trg, teacher_forcing_ratio=0.5):\n",
        "        # teacher_forcing_ratio 是什么 - 根据这个概率决定 下一个输出是依靠trg 还是实际输出\n",
        "        # src = [src len, batch size]\n",
        "        # src_len = [batch size]\n",
        "        # trg = [trg len, batch size]\n",
        "\n",
        "        trg_len = trg.shape[0]\n",
        "        batch_size = trg.shape[1]\n",
        "        vocab_size = len(self.vocab)\n",
        "        #         print('trg_len:',trg_len, 'batch_size: ',batch_size)\n",
        "\n",
        "        # 记录decoder结果，初始化\n",
        "        decoder_outputs = torch.zeros(trg_len, batch_size, vocab_size).to(self.device)\n",
        "\n",
        "        encoder_outputs, encoder_context, hidden, cell = self.encoder(src, src_len)\n",
        "        # encoder_outputs [src_len, batch_size, enc_hidden_dim]\n",
        "        # encoder_context [batch_size, enc_hidden_dim]\n",
        "\n",
        "        # 第一个是<sos>\n",
        "        target_word = trg[0, :]\n",
        "        # pad attention\n",
        "        mask = self.create_mask(src)\n",
        "        a = self.attention(encoder_context, encoder_outputs, mask)\n",
        "        # a [batch_size, src_len]\n",
        "        a = a.unsqueeze(1)\n",
        "        # a [batch_size, 1, src_len]\n",
        "        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n",
        "        # encoder_outputs [batch_size, src_len, enc_hidden_dim]\n",
        "\n",
        "        weighted_encoder_outputs = torch.bmm(a, encoder_outputs)  # batch 矩阵相乘，在有batch的情况下做矩阵相乘，不考虑batch\n",
        "        # weighted [batch_size, 1, enc_hidden_dim]\n",
        "        weighted_encoder_outputs = weighted_encoder_outputs.permute(1, 0, 2)\n",
        "        # weighted [1, batch_size, enc_hidden_dim]\n",
        "\n",
        "        ## QUESTION: decoder 将每个词拆开放入decoder 需要再想想逻辑\n",
        "        # decoder 输入的修改可能有误需检查\n",
        "        for t in range(1, trg_len):\n",
        "            outputs, hidden, cell = self.decoder(target_word,weighted_encoder_outputs, hidden, cell)\n",
        "\n",
        "            # output 是预测分布\n",
        "            decoder_outputs[t] = outputs\n",
        "\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "\n",
        "            # 最大可能的词\n",
        "            top1_word = outputs.argmax(1)\n",
        "            target_word = trg[t] if teacher_force else top1_word\n",
        "\n",
        "        return decoder_outputs"
      ],
      "id": "L2Ohycp0JFuB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0b764746"
      },
      "source": [
        "## 2.5 Train & Evaulation"
      ],
      "id": "0b764746"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f5a74ac"
      },
      "outputs": [],
      "source": [
        "# from torchtext.data.metrics import bleu_score\n",
        "def train(model, loader, optimizer, clip, loss_fn=nn.CrossEntropyLoss()):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for video_text, text_len, comment in tqdm(loader):\n",
        "        optimizer.zero_grad()\n",
        "        video_text = video_text.permute(1,0)\n",
        "        comment = comment.permute(1,0) # 原数据集为batch first, 因rnn需要，因此改成seq len first\n",
        "        \n",
        "        output = model(video_text,text_len, comment)\n",
        "#         print(output.shape)\n",
        "        # 变形，具体不太清楚 0默认为<sos> 不加入计算\n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        comment = comment[1:].contiguous().long().view(-1)\n",
        "        \n",
        "        loss = loss_fn(output, comment)\n",
        "        loss.backward()\n",
        "        \n",
        "        # 计算梯度的，具体不太清楚\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        # 具体也不太清楚\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    return epoch_loss / len(loader)"
      ],
      "id": "2f5a74ac"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aaaf370"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, loader,loss_fn=nn.CrossEntropyLoss()):\n",
        "    model.eval()\n",
        "    epoch_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for video_text, text_length, comment in tqdm(loader): \n",
        "            \n",
        "            video_text = video_text.permute(1,0)\n",
        "            comment = comment.permute(1,0) # 原数据集为batch first, 因rnn需要，因此改成seq len first\n",
        "        \n",
        "            output = model(video_text, text_length, comment)\n",
        "#         print(output.shape)\n",
        "\n",
        "            # 变形，具体不太清楚 0默认为<sos> 不加入计算\n",
        "            output_dim = output.shape[-1]\n",
        "\n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            comment = comment[1:].contiguous().long().view(-1)\n",
        "\n",
        "            loss = loss_fn(output, comment)\n",
        "            epoch_loss += loss.item()\n",
        "    return epoch_loss / len(loader)"
      ],
      "id": "1aaaf370"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b3ab7620"
      },
      "outputs": [],
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "id": "b3ab7620"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4z4OwVcyHMl"
      },
      "source": [
        "# 3. Train"
      ],
      "id": "v4z4OwVcyHMl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8f73723d"
      },
      "outputs": [],
      "source": [
        "# 训练参数\n",
        "INPUT_DIM = len(vocabulary)\n",
        "OUTPUT_DIM = len(vocabulary)\n",
        "\n",
        "ENCODER_EMBEDDED_DIM = 256\n",
        "DECODER_EMBEDDED_DIM = 256\n",
        "HIDDEN_DIM = 1024\n",
        "NUM_LAYERS = 2\n",
        "ENCODER_DROPOUT = 0.5\n",
        "DECODER_DROPOUT = 0.5\n",
        "\n",
        "# input_dim, embedding_dim, hidden_dim, rnn_layers, dropout_ratio\n",
        "encoder = Encoder(INPUT_DIM, ENCODER_EMBEDDED_DIM, HIDDEN_DIM, NUM_LAYERS,ENCODER_DROPOUT).to(device)\n",
        "attention = Attention(HIDDEN_DIM, HIDDEN_DIM).to(device)\n",
        "gated_attention = GatedAttention(HIDDEN_DIM).to(device)\n",
        "decoder = Decoder(OUTPUT_DIM, DECODER_EMBEDDED_DIM, HIDDEN_DIM, NUM_LAYERS, DECODER_DROPOUT,gated_attention).to(device)\n",
        "\n",
        "model = Seq2Seq(encoder, decoder, attention,vocabulary, device).to(device)"
      ],
      "id": "8f73723d"
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "        \n",
        "model.apply(init_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dsVa-gNTuG4",
        "outputId": "9dccc17f-40d2-4f7c-9b54-76d7773fa327"
      },
      "id": "7dsVa-gNTuG4",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(50437, 256)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (rnn): LSTM(256, 1024, num_layers=2)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(50437, 256)\n",
              "    (rnn): LSTM(256, 1024, num_layers=2)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "    (gated): GatedAttention(\n",
              "      (encoder_linear): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "      (decoder_linear): Linear(in_features=1024, out_features=1024, bias=False)\n",
              "      (attention): Linear(in_features=1024, out_features=1, bias=False)\n",
              "      (gate): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    )\n",
              "    (fcl): Linear(in_features=2048, out_features=50437, bias=True)\n",
              "  )\n",
              "  (attention): Attention(\n",
              "    (attention): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "    (v): Linear(in_features=1024, out_features=1, bias=False)\n",
              "  )\n",
              "  (vocab): Vocab()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "__tYQSvV0POt"
      },
      "outputs": [],
      "source": [
        "# model_path = os.path.join(WORK_DIR, 'Models','Pad+GateModel2', f'epoch5.pt')\n",
        "# results = torch.load(model_path)\n",
        "# model.load_state_dict(results['state_dict'])"
      ],
      "id": "__tYQSvV0POt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7256043",
        "outputId": "90cd94ef-8be2-4404-f938-88733215a243"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/8544 [00:00<?, ?it/s]/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)\n",
            "  import sys\n",
            "  8%|▊         | 704/8544 [44:12<8:12:08,  3.77s/it]"
          ]
        }
      ],
      "source": [
        "# 超参数\n",
        "# model 4 是带initweight\n",
        "EPOCHS = 10\n",
        "CLIP = 1\n",
        "# model_path = os.path.join(WORK_DIR, 'Models','Pad+GateModel2', f'epoch5.pt')\n",
        "# results = torch.load(model_path)\n",
        "# model.load_state_dict(results['state_dict'])\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "val_results = dict()\n",
        "train_results = dict()\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start_time = time.time()\n",
        "    train_loss = train(model, train_loader, optimizer= optimizer, clip=CLIP)\n",
        "    valid_loss = evaluate(model, valid_loader)\n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    result_dict = {\n",
        "        'train_loss':train_loss,\n",
        "        'valid_loss':valid_loss,\n",
        "        'state_dict': model.state_dict(),\n",
        "    }\n",
        "    \n",
        "    val_results[epoch+1] = valid_loss\n",
        "    train_results[epoch+1] = train_loss\n",
        "    if valid_loss < best_valid_loss:\n",
        "        torch.save(result_dict, os.path.join(WORK_DIR, 'Models','Pad+GateModel4', f'epoch{epoch+1}.pt'))\n",
        "        best_valid_loss = valid_loss\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "id": "c7256043"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9VDCVa6Fi9q"
      },
      "outputs": [],
      "source": [
        "torch.save(result_dict, os.path.join(WORK_DIR, 'Models','Pad+GateModel2', f'epoch1.pt'))"
      ],
      "id": "z9VDCVa6Fi9q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQqPOEXoFsl7"
      },
      "outputs": [],
      "source": [
        "print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')"
      ],
      "id": "RQqPOEXoFsl7"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnI7kaKcjocm"
      },
      "outputs": [],
      "source": [
        "result_df = pd.DataFrame({'train_loss':train_results,'valid_loss': val_results})\n",
        "result_df = result_df.reindex(sorted(train_results.keys()))\n",
        "result_df['model'] = 'Pad+GateModel2'\n",
        "result_df['train_ppl'] = result_df['train_loss'].apply(lambda x: math.exp(x))\n",
        "result_df['valid_ppl'] = result_df['valid_loss'].apply(lambda x: math.exp(x))\n",
        "\n",
        "result_df = result_df.reindex(columns = ['model', 'train_loss', 'valid_loss','train_ppl', 'valid_ppl'])\n",
        "result_df.to_csv(os.path.join(WORK_DIR, 'Models','Pad+GateModel2','results.csv'))"
      ],
      "id": "LnI7kaKcjocm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnnFxqlzjwQv"
      },
      "source": [
        "# 4. Evaluation"
      ],
      "id": "hnnFxqlzjwQv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "156TksOeC8ES"
      },
      "source": [
        "## 自定义testDataset"
      ],
      "id": "156TksOeC8ES"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8GseT0gC7CK"
      },
      "outputs": [],
      "source": [
        "class TestTextDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer=tokenizer, vocab=vocabulary, video_length=7000, comment_length = 32):\n",
        "        self.video_length = video_length\n",
        "        self.comment_length = comment_length\n",
        "        \n",
        "        self.tokenizer = tokenizer\n",
        "        self.vocab = vocab\n",
        "    \n",
        "        self.video_df = data['video']\n",
        "        self.comment_df = data['comment']\n",
        "\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        vid = self.comment_df.iloc[index]['vid']\n",
        "        \n",
        "        # trg\n",
        "        comment = str(self.comment_df.iloc[index]['en_content'])\n",
        "        tokenized_comment = self.tokenizer(comment)\n",
        "        tagged_comment = self.tag(tokenized_comment, self.comment_length)\n",
        "        comment_idxs = self.vocab.forward(tagged_comment)\n",
        "        \n",
        "        # src\n",
        "        video = self.video_df[self.video_df['vid'] == vid]\n",
        "        \n",
        "        title = str(video['title'].item())\n",
        "        transcript = str(video['transcript'].item())\n",
        "        \n",
        "        video_texts = ' '.join([title, transcript])\n",
        "        \n",
        "        tokenized_video_texts = self.tokenizer(video_texts)[:self.video_length-2]\n",
        "        text_length = self.video_length \n",
        "        \n",
        "        tagged_video_texts = self.tag(tokenized_video_texts, self.video_length)\n",
        "        \n",
        "        video_idxs = self.vocab.forward(tagged_video_texts)\n",
        "        \n",
        "        return torch.tensor(video_idxs), torch.tensor(text_length),torch.tensor(comment_idxs), vid\n",
        "        # return torch.tensor(video_idxs).to(device), torch.tensor(comment_idxs).to(device)\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.comment_df.shape[0]\n",
        "        \n",
        "    def tag(self, words, length):\n",
        "        words.insert(0, '<sos>')\n",
        "        words.append(\"<eos>\")\n",
        "        words = words + ['<pad>']*(length-len(words))\n",
        "        return words\n",
        "        "
      ],
      "id": "t8GseT0gC7CK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpFH4SmCj1D3"
      },
      "outputs": [],
      "source": [
        "model_path = os.path.join(WORK_DIR, 'Models','Pad+GateModel2', 'epoch8.pt')\n",
        "result = torch.load(model_path)\n",
        "model.load_state_dict(result['state_dict'])"
      ],
      "id": "gpFH4SmCj1D3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lPpaCBMNEurP"
      },
      "outputs": [],
      "source": [
        "def test_collate_fn(data):\n",
        "    data.sort(key=lambda x: x[1], reverse=True) \n",
        "    video = [row[0].numpy() for row in data]\n",
        "    length = [row[1] for row in data]\n",
        "    comment = [row[2].numpy() for row in data]\n",
        "    vid = [row[3]for row in data]\n",
        "    # return torch.Tensor(video).int().to(device), torch.Tensor(length).int().to(device), torch.Tensor(comment).int().to(device)\n",
        "    return torch.Tensor(video).int().to(device), torch.Tensor(length), torch.Tensor(comment).int().to(device), vid"
      ],
      "id": "lPpaCBMNEurP"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jwT4zxgQmkh9"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "test_dataset = TestTextDataset(test_data)\n",
        "test_loader = DataLoader(test_dataset, batch_size=128, collate_fn=test_collate_fn)\n",
        "ratio = 0\n",
        "with torch.no_grad():\n",
        "    while ratio < 0.8:\n",
        "        for video_texts, text_lens, comments, vid in tqdm(test_loader): \n",
        "            results = []\n",
        "            video_texts = video_texts.permute(1,0)\n",
        "            comments = comments.permute(1,0) # 原数据集为batch first, 因rnn需要，因此改成seq len first\n",
        "            outputs = model(video_texts,text_lens, comments, ratio)\n",
        "            outputs = outputs.permute(1, 0 ,2)\n",
        "            comments = comments.permute(1, 0)\n",
        "            \n",
        "            for i in range(outputs.shape[0]):\n",
        "                generated_comment_index = outputs[i,:,:].argmax(1)\n",
        "                generated_comment = vocabulary.lookup_tokens(list(generated_comment_index))\n",
        "                generated_comment = ' '.join([word for word in generated_comment if word not in ['<pad>', '<eos>', '<sos>']])\n",
        "                comment = vocabulary.lookup_tokens(list(comments[i]))\n",
        "                comment = ' '.join([word for word in comment if word not in ['<pad>', '<eos>', '<sos>']])\n",
        "                results.append(['PadGateModel',vid[i], str(ratio), comment, generated_comment, '\\n']) \n",
        "            with open(WORK_DIR+'Models/Pad+GateModel2/'+'predictions.csv', 'a') as f:\n",
        "                f.writelines([','.join(line) for line in results])\n",
        "        ratio += 0.1"
      ],
      "id": "jwT4zxgQmkh9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SouZUAJWKThI"
      },
      "outputs": [],
      "source": [
        "# 目前的prediction.csv 是 epoch 8"
      ],
      "id": "SouZUAJWKThI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7txxnXmnGo7b"
      },
      "outputs": [],
      "source": [
        "output_df = pd.DataFrame(results, columns=['vid', 'prediction', 'comment'])\n",
        "output_df['model'] = "
      ],
      "id": "7txxnXmnGo7b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVjzyIZdosFT"
      },
      "outputs": [],
      "source": [
        "folder =  os.path.join(WORK_DIR, 'Models','Pad+GateModel2')\n",
        "val_results = dict()\n",
        "train_results = dict()\n",
        "for file in os.listdir(folder):\n",
        "    epoch = int(file[5])\n",
        "    result = torch.load(os.path.join(folder, file))\n",
        "    val_results[epoch] = result['valid_loss']\n",
        "    train_results[epoch] =  result['train_loss']\n",
        "\n",
        "result_df = pd.DataFrame({'train_loss':train_results,'valid_loss': val_results})\n",
        "result_df = result_df.reindex(sorted(train_results.keys()))\n",
        "result_df['model'] = 'Pad+GateModel2'\n",
        "result_df['train_ppl'] = result_df['train_loss'].apply(lambda x: math.exp(x))\n",
        "result_df['valid_ppl'] = result_df['valid_loss'].apply(lambda x: math.exp(x))\n",
        "\n",
        "result_df = result_df.reindex(columns = ['model', 'train_loss', 'valid_loss','train_ppl', 'valid_ppl'])\n",
        "result_df.to_csv('results.csv')"
      ],
      "id": "oVjzyIZdosFT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fb-q_Ea6EtbK"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(result_df['train_ppl'], label='train ppl')\n",
        "plt.plot(result_df['valid_ppl'], label='valid ppl')\n",
        "plt.legend()"
      ],
      "id": "Fb-q_Ea6EtbK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeoawZE9Ek58"
      },
      "outputs": [],
      "source": [
        "folder =  os.path.join(WORK_DIR, 'Models','Pad+GateModel2')\n",
        "val_results = dict()\n",
        "train_results = dict()\n",
        "for file in os.listdir(folder):\n",
        "    epoch = int(file[5])\n",
        "    result = torch.load(os.path.join(folder, file))\n",
        "    model.load_state_dict(result['state_dict'])\n",
        "    valid_loss = evaluate(model, valid_loader)\n",
        "    print(f'epoch{epoch}: ')\n",
        "    print(f\"train loss:{result['train_loss']} | 0.5-valid loss:{result['valid_loss']} | 0-valid loss:{valid_loss}\" )"
      ],
      "id": "oeoawZE9Ek58"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "[MscProject]GatedAttentionModel.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python [conda env:deeplearning]",
      "language": "python",
      "name": "conda-env-deeplearning-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}